{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, AveragePooling1D, AveragePooling2D,TimeDistributed\n",
    "from keras.layers import LSTM, SpatialDropout1D, Merge\n",
    "from keras import backend\n",
    "np.random.seed(0)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train docs...\n",
      "dictionary size:  126016\n",
      "converting to token ids...\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "train = pd.read_csv('data/features/features.csv', sep=',', header=0)\n",
    "\n",
    "#y labels\n",
    "useful = train[\"useful\"]\n",
    "cancer_types_raw = train[\"cancer_types\"]\n",
    "cancer_types_raw = [re.sub(r'[^a-z, ]', '', s.lower()) for s in cancer_types_raw.values]\n",
    "cancer_types_raw = np.array([s.split(\",\") for s in cancer_types_raw])\n",
    "\n",
    "#text features\n",
    "text = train['abstract']+train['fulltitle']+train['subtitle']\n",
    "text = text.values.astype(dtype=str)\n",
    "\n",
    "#only useful examples have a cancer type\n",
    "idxthere = np.nonzero(train['useful'] == 1)[0]\n",
    "text = text[idxthere]\n",
    "cancer_types_raw = cancer_types_raw[idxthere]\n",
    "\n",
    "#prepare Y\n",
    "mlb = MultiLabelBinarizer()\n",
    "cancer_types = mlb.fit_transform(cancer_types_raw)\n",
    "    \n",
    "\n",
    "num_labels = len(np.unique(useful))\n",
    "\n",
    "#text pre-processing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stop_words.remove('but')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('no')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print(\"pre-processing train docs...\")\n",
    "processed_docs_train = []\n",
    "for doc in text:\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    processed_docs_train.append(stemmed)\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_docs_train)\n",
    "dictionary_size = len(dictionary.keys())\n",
    "print(\"dictionary size: \", dictionary_size)\n",
    "#dictionary.save('dictionary.dict')\n",
    "#corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print(\"converting to token ids...\")\n",
    "word_id_set, word_id_len = [], []\n",
    "for doc in processed_docs_train:\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_set.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))\n",
    "\n",
    "seq_len = np.round((np.mean(word_id_len) + 2*np.std(word_id_len))).astype(int)\n",
    "\n",
    "#pad sequences\n",
    "word_id_set = sequence.pad_sequences(np.array(word_id_set), maxlen=seq_len)\n",
    "\n",
    "#y_train_enc = np_utils.to_categorical(sentiment_train, num_labels)\n",
    "#y_train_enc = (sentiment_train-2)/2 #map down to: [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_id_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fe452c6d90c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#split train/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mword_id_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_id_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancer_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mword_id_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_id_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_id_train' is not defined"
     ]
    }
   ],
   "source": [
    "nb_classes = cancer_types.shape[1]\n",
    "\n",
    "#shuffle data set\n",
    "random_idx = np.arange(word_id_set.shape[0])\n",
    "np.random.shuffle(random_idx)\n",
    "word_id_set = word_id_set[random_idx,:]\n",
    "cancer_types = cancer_types[random_idx]\n",
    "\n",
    "#split train/test\n",
    "word_id_test = word_id_set[:1000,]\n",
    "y_test = cancer_types[:1000,]\n",
    "word_id_train = word_id_set[1000:,]\n",
    "y_train = cancer_types[1000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#LSTM (using text from abstract+title)\n",
    "model = Sequential()\n",
    "model.add(Embedding(dictionary_size, 128))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, recurrent_dropout=0.2, dropout=0.2))\n",
    "model.add(Dense(nb_classes, kernel_initializer=\"uniform\", activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "print(\"fitting LSTM ...\")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(word_id_train, y_train, epochs=1, batch_size=64, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#preds = model.predict_proba(X_test)[:,1]\n",
    "#test_pred = model.predict_classes(word_id_test)\n",
    "predicted = model.predict_classes(word_id_test)\n",
    "sk.metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_labels = mlb.inverse_transform(predicted)\n",
    "true_labels = mlb.inverse_transform(y_test)\n",
    "#print(true_labels)\n",
    "for i in range(0,100):\n",
    "    print(pred_labels[i])\n",
    "    print(true_labels[i])\n",
    "    print(\"################\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
