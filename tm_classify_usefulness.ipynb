{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/timomoeller/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timomoeller/.virtualenvs/keras/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfiltered data, num samples: 45885 with num features: 22\n",
      "filtered data with samples: 45885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%matplotlib inline\n",
    "#import xgboost\n",
    "\n",
    "train = pd.read_csv(\"../data/features/features.csv\",\n",
    "                    header=0,delimiter=\",\",quotechar='\"',error_bad_lines=False)\n",
    "\n",
    "print(\"unfiltered data, num samples: %i with num features: %i\" %(train.shape[0],train.shape[1]))\n",
    "\n",
    "train.fillna(' ',inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "print(\"filtered data with samples: %i\" %(train.shape[0]))\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do publishers have slightly different spelling??\n",
    "# looking at string distance of publishers. result: pretty sure unique journals, no spelling diffs\n",
    "# small string distances, but only for abbreviated pubs eg. AAM journal, APM journal -> they need to be different\n",
    "uniPubs = np.unique(pubs)\n",
    "uniPubs = np.array([x for x in uniPubs if len(x) > 5]) # filtering out unkown\n",
    "print(uniPubs.shape)\n",
    "\n",
    "import Levenshtein\n",
    "d = Levenshtein.distance\n",
    "ldist = np.zeros((uniPubs.shape[0],uniPubs.shape[0]))\n",
    "for i in range(uniPubs.shape[0]):\n",
    "    for j in range(i,uniPubs.shape[0]):\n",
    "        #ldist[i,j] = d(uniPubs[i],uniPubs[j]) # commented out, takes about 40s\n",
    "\n",
    "#manually check similar pubs. \n",
    "current = 7\n",
    "print(np.sort(ldist[current])[:10])\n",
    "print(np.argsort(ldist[current])[:10])\n",
    "print(uniPubs[current] + '  ' + uniPubs[1328])\n",
    "plt.hist(ldist.flatten(),bins=40,range=[1,400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PREPRocessing\n",
    "\n",
    "#y labels\n",
    "useful = train[\"useful\"]\n",
    "\n",
    "#features\n",
    "abstracts = train['abstract'].values.astype(dtype=str)\n",
    "title = train['fulltitle'].values.astype(dtype=str)\n",
    "journal = train['subtitle'].values.astype(dtype=str)\n",
    "num = train['num']\n",
    "\n",
    "# #only rows with abstract\n",
    "# idxthere = np.nonzero(train['abstract'] != ' ')[0]\n",
    "# print(\"exluding %i rows because of empty abstract\" %idxthere.shape[0])\n",
    "# title = title[idxthere]\n",
    "# journal = journal[idxthere]\n",
    "# abstracts = abstracts[idxthere]\n",
    "# useful = useful[idxthere]\n",
    "\n",
    "#one hot encoded journals\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(vocabulary=np.unique(journal),binary=True)\n",
    "hot_journal = vect.fit_transform(journal)\n",
    "#only keep journals with more than 100 occurences\n",
    "journal_count = np.sum(hot_journal, axis=0)\n",
    "idx_imp_journals = np.nonzero(journal_count > 20)[0]\n",
    "imp_journals = hot_journal[:,idx_imp_journals]\n",
    "\n",
    "#bag of words for abstracts and titles\n",
    "reRemoved = [re.sub(r'[^a-z ]', '', s.lower()) for s in abstracts]\n",
    "vectText = CountVectorizer(max_df=0.6,min_df=10,stop_words=stopwords.words(\"english\"))\n",
    "bow_text = vectText.fit_transform(reRemoved)\n",
    "\n",
    "vect = CountVectorizer(max_df=0.6,min_df=4,stop_words=stopwords.words(\"english\"))\n",
    "reRemoved = [re.sub(r'[^a-z ]', '', s.lower()) for s in title]\n",
    "bow_title = vect.fit_transform(reRemoved)\n",
    "\n",
    "#LDA on abstracts bow !!!!! DOESN't improve performance\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=50,n_jobs=1)\n",
    "lda.partial_fit(bow_text)\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "topicDist = lda.transform(bow_text)\n",
    "\n",
    "\n",
    "#combine features\n",
    "features = sp.sparse.hstack((bow_title, bow_text, imp_journals)).tocsr() # AUC 0.822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9177,)\n",
      "MSE 0.171\n",
      "AUC: 0.822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, useful, test_size=0.2, random_state=42)\n",
    "# classification\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(max_depth=10,n_estimators=400)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(preds.shape)\n",
    "print(\"MSE %.3f\" %(np.mean(np.power(preds - y_test,2))))\n",
    "print(\"AUC: %.3f\" %sk.metrics.roc_auc_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.189\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fsk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0d499db9b64b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC: %.3\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mfsk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fsk' is not defined"
     ]
    }
   ],
   "source": [
    "# classification on dataset consisting of true zero and one labels\n",
    "import xgboost as xgb\n",
    "\n",
    "true_df = pd.read_csv(\"../data/features/features-hodenniere.csv\",header=0,delimiter=\",\",quotechar='\"',error_bad_lines=False)\n",
    "true_num = true_df[\"num\"]\n",
    "msk = np.in1d(num,true_num)\n",
    "\n",
    "X_train = features[~msk]\n",
    "X_test = features[msk]\n",
    "y_train = useful[~msk]\n",
    "y_test = useful[msk]\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=10,n_estimators=400)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"MSE %.3f\" %(np.mean(np.power(preds - y_test,2))))\n",
    "print(\"AUC: %.3f\" %sk.metrics.roc_auc_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.796\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC: %.3f\" %sk.metrics.roc_auc_score(y_test,preds))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
